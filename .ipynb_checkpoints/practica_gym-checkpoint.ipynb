{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089b8b8b-c4fe-423c-b83d-f845a138e273",
   "metadata": {},
   "source": [
    "# Lab 3 - Reinforcement Learning\n",
    "## Cliff Walking\n",
    "Marta Simón, Ana Rotella, Blanca Gómez, Enrique Gil and María José Medina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf142e4b-767d-4ca4-b7f7-1aec53fdeea1",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e0652-733e-4f70-877d-ac8833f6a0d7",
   "metadata": {},
   "source": [
    "The board is a 4x12 matrix, with (using NumPy matrix indexing):\n",
    "\n",
    "- [3, 0] as the start at bottom-left\n",
    "\n",
    "- [3, 11] as the goal at bottom-right\n",
    "\n",
    "- [3, 1..10] as the cliff at bottom-center\n",
    "\n",
    "If the agent steps on the cliff, it returns to the start. An episode terminates when the agent reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0fe191-e62d-4702-a4c1-a71006ee940f",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13914fc5-8a47-4897-bb37-7f67259f16e0",
   "metadata": {},
   "source": [
    "There are 4 discrete deterministic actions:\n",
    "\n",
    "- 0: move up\n",
    "\n",
    "- 1: move right\n",
    "\n",
    "- 2: move down\n",
    "\n",
    "- 3: move left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c03196-ee63-4510-bcf6-adc343a8ec39",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b149ca7-7c0f-4d2e-a2bb-3382fe7da548",
   "metadata": {},
   "source": [
    "There are 3x12 + 1 possible states. In fact, the agent cannot be at the cliff, nor at the goal (as this results in the end of the episode). It remains all the positions of the first 3 rows plus the bottom-left cell. The observation is simply the current position encoded as flattened index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af0104-c415-44cc-8583-2c4b42a491fa",
   "metadata": {},
   "source": [
    "### Recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad54a95-7a5b-43f7-93dc-cbe1b4dc5d4b",
   "metadata": {},
   "source": [
    "Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5377f-9724-4d9b-b981-84b35a1525d6",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310cc18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maria\\anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Maria\\anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\Maria\\anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "import matplotlib\n",
    "import random\n",
    "import itertools\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
    "import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc51fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da2a35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########  Using gym\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b4010-2030-4ff4-aea0-daa2c9635e47",
   "metadata": {},
   "source": [
    "First, we create the Cliff Walking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614be7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the gym environment\n",
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35824c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the environment with reset\n",
    "env.seed(42)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bdf15e-d046-41ca-bce1-f75a1a5f9a43",
   "metadata": {},
   "source": [
    "Then, we check that the number of states and actions coincides with the environment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc87833-a978-4eea-aa19-8619ef801352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 48\n",
      "Number of actions that an agent can take: 4\n"
     ]
    }
   ],
   "source": [
    "# 4x12 grid = 48 states\n",
    "print (\"Number of states:\", env.nS)\n",
    "# either go left, up, down or right\n",
    "print (\"Number of actions that an agent can take:\", env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ff829-2d6e-490e-8852-0a8f0d331834",
   "metadata": {},
   "source": [
    "We then check in which position of the board the agent is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5210de42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8cac0-1425-460a-bc76-aa8035a8edf2",
   "metadata": {},
   "source": [
    "Since the position is represented using a flatmap, obs = 36 means that the agent is at position 36 in the board, that is, the start.\n",
    "We then plot the environment to check that the assumption is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81fb6979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An environment can be visualized by calling its render()\n",
    "img = env.render()\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b2cda-a0c5-421b-a5f5-514025160754",
   "metadata": {},
   "source": [
    "The x represents the agent and the T the goal postion, Therefore, the agent is in fact at the start.\n",
    "One can also check where the agent is and what are its options with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfff83bd-6789-4c28-ba60-ec4262f900f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state 36\n",
      "Transitions from current state: {0: [(1.0, 24, -1, False)], 1: [(1.0, 36, -100, False)], 2: [(1.0, 36, -1, False)], 3: [(1.0, 36, -1, False)]}\n"
     ]
    }
   ],
   "source": [
    "# Where am I? -> in \"x\" state\n",
    "print (\"Current state\", env.s)\n",
    "# What are my options? -> 4 action\n",
    "print (\"Transitions from current state:\", env.P[env.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f086c163-2ec5-4d92-82ff-1c5d668db1bc",
   "metadata": {},
   "source": [
    "The dictionary above represents:\n",
    "\n",
    "    action : (prob, obs, reward, done)\n",
    "- action : possible action that the agent can take\n",
    "- obs : observation were the agent to take the action\n",
    "- reward : reward were the agent to take the action\n",
    "- done : True if the agent would reach its goal in the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba2fe8-f9c2-4c29-8671-ffd5a9fa7363",
   "metadata": {},
   "source": [
    "Now we will make the agent take an action (move up) to see how the environment changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "150623c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: 24\n",
      "Reward: -1\n",
      "Done: False\n",
      "Info: {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "action = 0  # move up\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(f\"Observation: {obs}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Done: {done}\")\n",
    "print(f\"Info: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd3b037-1625-4372-955f-d9553190e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abecfa-2c08-4705-87a0-0db9bfbd7201",
   "metadata": {},
   "source": [
    "The x has moved up one position. However, if the agent were to move right in stead of left, the x would not move as the positions in the cliff are not feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abd105d8-8b24-4efc-88ad-a75b3ff2c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: 36\n",
      "Reward: -100\n",
      "Done: False\n",
      "Info: {'prob': 1.0}\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 1  # move up\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(f\"Observation: {obs}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Done: {done}\")\n",
    "print(f\"Info: {info}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7674b8-6555-47d6-8ecb-4304d66ed722",
   "metadata": {},
   "source": [
    "### Basic Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949530ac-a864-4eb5-9109-6ab657fa05ae",
   "metadata": {},
   "source": [
    "Walk to the goal without falling into the cliff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d210dbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.seed(42)\n",
    "\n",
    "def basic_policy(obs):\n",
    "    action = 1 # By default go right\n",
    "    if obs >= 36: # If the agent is in the lower row, go up\n",
    "        action = 0\n",
    "    elif (obs == 11) or ( obs == 23) or ( obs == 35): # If the agent is in the last column, go down\n",
    "        action = 2\n",
    "    return action\n",
    "\n",
    "totals = []\n",
    "# for episode in range(500):\n",
    "for episode in range(1):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    # for step in range(200):\n",
    "    for step in range(13):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcca04f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-13.0, 0.0, -13, -13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)\n",
    "\n",
    "# Well, as expected, this strategy is a bit too basic: the best it did was to keep the pole up for only 68 steps. \n",
    "# This environment is considered solved when the agent keeps the pole up for 200 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fd565f7-cd91-4088-a541-5031f4bb1928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58255bd-51be-41d8-9beb-950ac9c1f489",
   "metadata": {},
   "source": [
    "## Sarsa (On-policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b5dbbc-9949-42d2-b054-e4d599d4937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        pi = np.ones(nA, dtype=float) * (epsilon/nA)\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        pi[best_action] += (1.0 - epsilon)\n",
    "        return pi\n",
    "    \n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88b9d84c-c328-43a2-961e-42866b10611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of episode: 291847\n",
      "CPU times: total: 7.69 s\n",
      "Wall time: 7.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "next_state = obs\n",
    "\n",
    "# generate a episode in cliffwalking environment for above sample_policy\n",
    "def generate_episode(policy, verbose=False):\n",
    "    episode = []\n",
    "    env = CliffWalkingEnv()\n",
    "    curr_state = env.reset()\n",
    "    probs = policy(curr_state)\n",
    "    action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "    \n",
    "    while True:\n",
    "        if verbose:\n",
    "            print (\"Current observation:\")\n",
    "            print (\"Current poistion:\", curr_state)\n",
    "            #print (env.render())\n",
    "        \n",
    "        next_obs, reward, is_terminal, _ = env.step(action)\n",
    "        \n",
    "        if verbose:\n",
    "            print (\"Action taken:\", actions[action])\n",
    "            print (\"Next observation:\", next_obs)\n",
    "            print (\"Reward recieved:\", reward)\n",
    "            print (\"Terminal state:\", is_terminal)\n",
    "            #print (env.render())\n",
    "            print (\"-\"*20)\n",
    "        episode.append((curr_state, action, reward))\n",
    "        \n",
    "        # Pick the next action\n",
    "        next_probs = policy(next_state)\n",
    "        next_action = np.random.choice(np.arange(len(next_probs)), p=next_probs)\n",
    "    \n",
    "        curr_state = next_obs\n",
    "        action = next_action\n",
    "\n",
    "        if (is_terminal):\n",
    "            break\n",
    "\n",
    "    return episode\n",
    "    \n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "policy = make_epsilon_greedy_policy(Q, 0.1, env.action_space.n)\n",
    "e = generate_episode(policy)\n",
    "#print (\"Episode:\", e)\n",
    "print (\"Length of episode:\", len(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92325de0-bdab-48cc-a1d2-744fb009a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        probs = policy(state)\n",
    "        action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "        \n",
    "        # Approach : 1\n",
    "        # Takes a lot a lot lot of time to run\n",
    "        # ep = generate_episode(policy)\n",
    "        # for i in range(len(ep)-1):\n",
    "        #     state, action, reward = ep[i]\n",
    "        #     next_state, next_action, next_reward = ep[i+1]\n",
    "        \n",
    "        #     td_target = reward + discount_factor * Q[next_state][next_action]\n",
    "        #     td_error = td_target - Q[state][action]\n",
    "        #     Q[state][action] += alpha * td_error\n",
    "\n",
    "        #     stats.episode_rewards[i_episode] += reward\n",
    "        #     stats.episode_lengths[i_episode] = t\n",
    "\n",
    "        # Approach : 2\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            # Take a step\n",
    "            next_state, reward, is_terminal, _ = env.step(action)\n",
    "            \n",
    "            # Pick the next action\n",
    "            next_probs = policy(next_state)\n",
    "            next_action = np.random.choice(np.arange(len(next_probs)), p=next_probs)\n",
    "            \n",
    "            td_target = reward + discount_factor * Q[next_state][next_action]\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_error\n",
    "\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            if is_terminal:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff6a6d13-9642-4d45-8154-c706d6da3641",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'plotting' has no attribute 'EpisodeStats'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m, in \u001b[0;36msarsa\u001b[1;34m(env, num_episodes, discount_factor, alpha, epsilon)\u001b[0m\n\u001b[0;32m     20\u001b[0m Q \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: np\u001b[38;5;241m.\u001b[39mzeros(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Keeps track of useful statistics\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mplotting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEpisodeStats\u001b[49m(\n\u001b[0;32m     24\u001b[0m     episode_lengths\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(num_episodes),\n\u001b[0;32m     25\u001b[0m     episode_rewards\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(num_episodes))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# The policy we're following\u001b[39;00m\n\u001b[0;32m     28\u001b[0m policy \u001b[38;5;241m=\u001b[39m make_epsilon_greedy_policy(Q, epsilon, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'plotting' has no attribute 'EpisodeStats'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, stats = sarsa(env, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957e405-5db5-45f6-9c63-2104209c3246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
