{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.01\n",
    "gamma = 0.99\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "memory = deque(maxlen=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el entorno\n",
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # == env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# Define la red neuronal\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])\n",
    "\n",
    "# Compila el modelo\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para seleccionar una acción\n",
    "def select_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return tf.argmax(model.predict(state)[0]).numpy()\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    # Obtén una muestra aleatoria de la memoria\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    \n",
    "    for state, action, reward, next_state, done in batch:\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + gamma * tf.reduce_max(model.predict(next_state)[0]).numpy()\n",
    "        \n",
    "        target_f = model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        \n",
    "        # Entrena el modelo con la muestra\n",
    "        model.fit(state, target_f, epochs=1, verbose=0)\n",
    "    \n",
    "    # Reduce la probabilidad de seleccionar una acción aleatoria\n",
    "    global epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m      4\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 5\u001b[0m     state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(state, [\u001b[38;5;241m1\u001b[39m, \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m time_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m      8\u001b[0m         env\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "episode_scores = []\n",
    "# Entrena al agente\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    state = tf.reshape(state, [1, env.observation_space.shape[0]])\n",
    "    \n",
    "    for time_step in range(500):\n",
    "        env.render()\n",
    "        \n",
    "        action = select_action(state)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = tf.reshape(next_state, [1, env.observation_space.shape[0]])\n",
    "        \n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "        train_model()\n",
    "\n",
    "        episode_scores.append(time_step+1)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episodio {} completado en {} pasos. Epsilon: {:.2}\".format(episode, time_step+1, epsilon))\n",
    "            break\n",
    "        \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pinta el gráfico\n",
    "plt.plot(episode_scores)\n",
    "plt.title('Aprendizaje del agente')\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Puntaje total')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Del profe, revisar si funciona\n",
    "env.seed(42)\n",
    "state = env.reset()\n",
    "\n",
    "frames = []\n",
    "\n",
    "for step in range(200):\n",
    "    action = epsilon_greedy_policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    \n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.6.0\n",
      "  Using cached matplotlib-3.6.0-cp310-cp310-win_amd64.whl (7.2 MB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib==3.6.0) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib==3.6.0) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib==3.6.0) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib==3.6.0) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib==3.6.0) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib==3.6.0) (9.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.6.0) (1.16.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.1\n",
      "    Uninstalling matplotlib-3.7.1:\n",
      "      Successfully uninstalled matplotlib-3.7.1\n",
      "Successfully installed matplotlib-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib==3.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_3' (type Sequential).\n    \n    Input 0 of layer \"dense_9\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (32,)\n    \n    Call arguments received by layer 'sequential_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(32,), dtype=int32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m next_states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(batch[:, \u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     58\u001b[0m dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(batch[:, \u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m---> 60\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m q_next \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(model\u001b[38;5;241m.\u001b[39mpredict(next_states), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     62\u001b[0m targets[np\u001b[38;5;241m.\u001b[39marange(batch_size), actions] \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones) \u001b[38;5;241m*\u001b[39m q_next\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewf1tavql.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Maria\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_3' (type Sequential).\n    \n    Input 0 of layer \"dense_9\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (32,)\n    \n    Call arguments received by layer 'sequential_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(32,), dtype=int32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Define los parámetros del modelo\n",
    "n_episodes = 100         # número de episodios para entrenar\n",
    "n_steps = 100             # número máximo de pasos por episodio\n",
    "memory_size = 10000       # tamaño máximo de la memoria de repetición\n",
    "batch_size = 32           # tamaño del lote para el aprendizaje por lotes\n",
    "gamma = 0.95              # factor de descuento para las recompensas\n",
    "epsilon_start = 1.0       # valor inicial de epsilon para la exploración\n",
    "epsilon_end = 0.01        # valor final de epsilon para la exploración\n",
    "epsilon_decay = 0.995     # tasa de decaimiento de epsilon por episodio\n",
    "\n",
    "# Inicializa la memoria de repetición\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "# Inicializa el valor de epsilon\n",
    "epsilon = epsilon_start\n",
    "\n",
    "# Itera a través de los episodios de entrenamiento\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    # Resetea el entorno para el nuevo episodio\n",
    "    state = np.array(env.reset())\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Itera a través de los pasos del episodio\n",
    "    for step in range(n_steps):\n",
    "        \n",
    "        # Elige una acción usando la política epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            \n",
    "            q_values = model.predict(state.reshape(1, -1))\n",
    "            action = np.argmax(q_values[0])\n",
    "            \n",
    "        # Ejecuta la acción y observa el resultado\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.array(next_state)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Añade la experiencia a la memoria de repetición\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Actualiza el estado actual\n",
    "        state = next_state\n",
    "        \n",
    "        # Realiza el aprendizaje por lotes si la memoria de repetición es lo suficientemente grande\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = np.array(random.sample(memory, batch_size))\n",
    "            states = np.array(batch[:, 0].tolist())\n",
    "            actions = np.array(batch[:, 1].tolist())\n",
    "            rewards = np.array(batch[:, 2].tolist())\n",
    "            next_states = np.array(batch[:, 3].tolist())\n",
    "            dones = np.array(batch[:, 4].tolist())\n",
    "            \n",
    "            targets = model.predict(states)\n",
    "            q_next = np.amax(model.predict(next_states), axis=1)\n",
    "            targets[np.arange(batch_size), actions] = rewards + gamma * (1 - dones) * q_next\n",
    "            \n",
    "            model.fit(states, targets, epochs=1, verbose=0)\n",
    "            \n",
    "        # Sal del bucle si el episodio ha terminado\n",
    "        if done:\n",
    "            break\n",
    "      # Lista para almacenar las sumas de las recompensas por episodio\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Sal del bucle si el episodio ha terminado\n",
    "    if done:\n",
    "    # Agrega la suma de las recompensas para este episodio a la lista\n",
    "        episode_rewards.append(total_reward)\n",
    "    # Imprime los resultados del episodio\n",
    "        print(\"Episodio {}, Total Reward: {}, Epsilon: {:.4f}\".format(episode, total_reward, epsilon))\n",
    "\n",
    "    # Imprime la suma de las recompensas para los últimos 10 episodios\n",
    "    if episode > 0 and episode % 10 == 0:\n",
    "        print(\"Suma de recompensas de los últimos 10 episodios:\", sum(episode_rewards[-10:]))\n",
    "\n",
    "            \n",
    "    # Reduce el valor de epsilon después de cada episodio\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Imprime los resultados del episodio\n",
    "    print(\"Episodio {}, Total Reward: {}, Epsilon: {:.4f}\".format(episode, total_reward, epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\marti\\anaconda3\\envs\\dl\\lib\\site-packages (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
